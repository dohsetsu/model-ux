# Evaluating AI agents: A practical primer

**Author:** Jason Bice  
**Date:** December 2025  
**Purpose:** Foundation knowledge for understanding, building, and interpreting AI evaluations

---

## What is an eval?

An **eval** (evaluation) is a systematic way to measure whether an AI system is doing its job well.

Think of it like a test — but instead of testing a student, you're testing an AI agent's outputs against some standard of "good."

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Input     │ ──► │  AI Agent   │ ──► │   Output    │
│  (question) │     │             │     │  (answer)   │
└─────────────┘     └─────────────┘     └─────────────┘
                                              │
                                              ▼
                                        ┌─────────────┐
                                        │    Eval     │
                                        │  (grading)  │
                                        └─────────────┘
                                              │
                                              ▼
                                        ┌─────────────┐
                                        │   Score     │
                                        │ (pass/fail) │
                                        └─────────────┘
```

---

## Why do evals matter?

Without evals, you're flying blind:

| Without Evals | With Evals |
|---------------|------------|
| "I think the agent is working well" | "The agent passes 87% of test cases" |
| "Users seem happy" | "Correctness is 92%, but completeness is only 68%" |
| "Let's ship it and see" | "This change improved relevance by 12%" |
| "Something broke but I don't know what" | "Regression detected: voice_tone dropped 15%" |

**Evals turn vibes into data.**

---

## Required components of an eval

Every eval needs these five things:

### 1. Test cases
The inputs you'll feed to the AI agent.

```
Example test case:
- User query: "What was my net profit last month?"
- Context: User is logged into QBO, has active P&L data
```

**Good test cases are:**
- Representative of real usage
- Diverse (cover edge cases, not just happy paths)
- Reproducible (same input → same test)

### 2. Ground truth
The "correct" answer to compare against.

```
Example ground truth:
"Your net profit for November 2024 was $12,450.00"
```

**Ground truth can be:**
- Human-written (gold standard, expensive)
- Extracted from source systems (e.g., actual QBO data)
- Generated by a reference model (faster, less reliable)

⚠️ **Ground truth quality determines eval quality.** Garbage in, garbage out.

### 3. Metrics
What dimensions of quality you're measuring.

| Metric | What it measures |
|--------|------------------|
| Correctness | Is the answer factually accurate? |
| Completeness | Does it fully address the question? |
| Relevance | Is the information useful and on-topic? |
| Voice/Tone | Does it sound appropriate for the context? |
| Latency | How fast did it respond? |
| Cost | How much did the API calls cost? |

**Choose metrics that matter for your use case.** A chatbot and a financial analyst tool need different metrics.

### 4. Evaluation method
How you determine pass/fail for each metric.

| Method | How it works | Pros | Cons |
|--------|--------------|------|------|
| **Exact match** | Output must equal ground truth exactly | Fast, deterministic | Too rigid for natural language |
| **Fuzzy match** | Semantic similarity above threshold | Handles paraphrasing | Threshold is arbitrary |
| **Rule-based** | Regex, keyword checks, format validation | Precise for specific patterns | Brittle, high maintenance |
| **LLM-as-a-judge** | Another AI grades the output | Flexible, nuanced | Expensive, can be inconsistent |
| **Human review** | People grade outputs | Gold standard for quality | Slow, expensive, doesn't scale |

### 5. Rubrics (for LLM-as-a-judge)
Instructions that tell the judge how to grade.

```
Example rubric for Correctness:
"Score TRUE if the response contains factually accurate information 
that matches the ground truth. Minor rounding differences are acceptable. 
Score FALSE if there are factual errors, wrong numbers, or contradictions."
```

**Rubrics are the soul of your eval.** Vague rubrics → inconsistent results.

---

## Types of evals

### By automation level

| Type | Description | When to use |
|------|-------------|-------------|
| **Manual** | Humans review every output | Early development, high-stakes decisions |
| **Semi-automated** | LLM grades, humans spot-check | Regular testing, moderate volume |
| **Fully automated** | No human in the loop | CI/CD pipelines, high volume |

### By timing

| Type | Description | When to use |
|------|-------------|-------------|
| **Offline** | Run on historical data, batch mode | Before deployment, regression testing |
| **Online** | Run on live traffic, real-time | Production monitoring, A/B tests |

### By scope

| Type | Description | Example |
|------|-------------|---------|
| **Unit eval** | Test one component in isolation | "Does the date parser work?" |
| **Integration eval** | Test components working together | "Does BI + QBO data return correct P&L?" |
| **End-to-end eval** | Test full user journey | "Can a user get their tax summary?" |

---

## The eval crisis (an industry-wide problem)

The AI industry is grappling with what researchers call the **LLM Evaluation Crisis** — a fundamental challenge in reliably assessing what large language models can actually do. This isn't a problem with any specific eval or team; it's a systemic issue that the entire field is working to solve.

Understanding this crisis is essential for Model UX practitioners because it shapes how we should think about building, interpreting, and improving evaluations.

### The core problem

Traditional software testing assumes deterministic behavior: same input → same output. LLMs break this assumption. They're non-deterministic by design, producing different outputs for the same prompt. This makes conventional testing approaches unreliable.

### Key dimensions of the crisis

#### 1. Non-determinism
LLMs produce different outputs for the same prompt across runs. This isn't a bug — it's how they work. But it means:
- The same test case can pass today and fail tomorrow
- "Flaky" results aren't always a sign of bad tests
- Statistical approaches are necessary, but add complexity

#### 2. Benchmark overfitting
Models can learn to perform well on public benchmarks without truly understanding the underlying task. They may:
- Memorize patterns from training data that overlap with test sets
- Generate answers that "look right" without genuine reasoning
- Score high on benchmarks but fail on slight variations

**Why it matters:** A model that "passes" an eval may still fail in real-world scenarios that weren't in the test set.

#### 3. Lack of self-awareness
LLMs often can't identify their own limitations. They may:
- Provide confident answers to questions they don't actually know
- Fail to signal uncertainty when they should
- "Hallucinate" plausible-sounding but incorrect information

**Why it matters:** An eval that only checks "is this answer correct?" misses "does the model know when it doesn't know?"

#### 4. Multi-turn inconsistency
Models may fail to maintain consistent reasoning across a conversation:
- Contradicting earlier statements
- Abandoning correct answers when challenged
- Losing context as conversations grow longer

**Why it matters:** Single-turn evals don't catch problems that only emerge in realistic multi-step interactions.

#### 5. Fragile reasoning
Models may rely on superficial patterns rather than deep understanding:
- Getting the right answer for the wrong reasons
- Failing when problems are rephrased
- Showing "brittleness" under minor perturbations

**Why it matters:** High eval scores can mask fundamental reasoning gaps.

#### 6. Evaluation difficulty itself
Scoring natural language is inherently hard:
- "Good" is subjective and context-dependent
- Benchmarks lag behind rapidly evolving capabilities
- Using LLMs to judge LLMs introduces its own biases

### What this means for practitioners

The eval crisis doesn't mean evals are useless — it means we need to be thoughtful about:

1. **Designing evals** that account for non-determinism and test real-world scenarios
2. **Interpreting results** with appropriate skepticism (high scores ≠ solved problem)
3. **Building rubrics** that match the specific context and use case
4. **Combining methods** rather than relying on any single approach

This is why Model UX exists as a discipline: we need people who understand both the language/UX side and the evaluation/measurement side to bridge this gap.

---

## How to read eval results

### Don't just look at the top-line number

❌ "We got 87% — ship it!"

✅ "We got 87% overall. Let's break that down..."

### Break down by dimension

| Metric | Score | Interpretation |
|--------|-------|----------------|
| Correctness | 92% | Strong — factual accuracy is good |
| Completeness | 68% | **Weak — many partial answers** |
| Relevance | 85% | Acceptable |
| Voice/Tone | 71% | **Needs attention** |

### Break down by segment

| Segment | Correctness | Notes |
|---------|-------------|-------|
| Simple queries | 96% | Easy cases handled well |
| Multi-part questions | 78% | **Struggles with complexity** |
| Edge cases | 61% | **Known weakness** |

### Look at the failures

The most valuable data is in the FALSE rows:
- What patterns do you see?
- Are failures random or systematic?
- Is the eval wrong, or is the agent wrong?

### Check for noise

Ask yourself:
- Do similar responses get similar scores?
- If you re-ran the eval, would results change significantly?
- Are there obvious contradictions in the reasoning?

---

## How to adjust evals

### When to adjust rubrics

✅ **Good reasons:**
- Rubric doesn't match user expectations
- Rubric creates perverse incentives
- Agent has legitimate domain-specific needs

❌ **Bad reasons:**
- "We need to hit 85%"
- "The failures look bad on the dashboard"
- "Other team is complaining"

### How to adjust rubrics

1. **Document the problem** — specific examples of unfair failures
2. **Propose specific changes** — not "make it easier" but "accounting terms should be acceptable when..."
3. **Test the change** — run both rubrics, compare results
4. **Validate with humans** — does the new rubric match expert judgment?
5. **Monitor for regression** — did you accidentally break something else?

### Agent-aware adjustments

Instead of one rubric for all agents:

```
IF agent == "Business_Intelligence":
    Apply BI-specific rubric
    - Accounting terminology: EXPECTED (not jargon)
    - Structured format: ACCEPTABLE
    - Comprehensive detail: GOOD (not verbose)
ELSE:
    Apply standard rubric
```

This lets each agent be judged by appropriate standards while keeping the eval framework unified.

---

## How to interpret results

### Correlation ≠ Causation

"We changed the prompt and correctness went up 5%"

But did the prompt change cause it? Or:
- Did test data change?
- Did the judge model change?
- Is it just noise?

**Always compare apples to apples:** same test cases, same judge, same rubrics.

### Beware of ceiling effects

If you're at 95% correctness, a 1% improvement is actually huge — you fixed 20% of your remaining errors.

If you're at 60%, a 5% improvement might just be noise.

### Trust but verify

LLM-as-a-judge is a tool, not an oracle.

**Spot-check the reasoning:**
- Does the judge's explanation make sense?
- Would a human agree with this verdict?
- Are there obvious errors in the evaluation?

### Results are a starting point, not an endpoint

Eval results tell you *where* to look, not *what* to do.

```
Eval says: "Completeness is failing"
     ↓
Investigation: "50% of failures are date interpretation issues"
     ↓
Root cause: "Agent and ground truth define 'last quarter' differently"
     ↓
Fix: "Standardize date interpretation logic"
```

---

## Quick reference: Eval red flags

| Red Flag | What it might mean |
|----------|-------------------|
| All metrics improved after "tuning" | Judge was made more lenient, not agents improved |
| Different agents have identical improvements | Global change, not targeted fixes |
| Same content gets different verdicts | Eval has noise/inconsistency |
| Correctness unchanged but subjective metrics jumped | Rubrics loosened, facts untouched |
| Team can't explain what changed | Changes weren't documented or understood |
| Failures dismissed as "eval problems" | Culture of explaining away rather than fixing |
| No human validation of judge | Blind trust in automated grading |

---

## Summary

1. **Evals are essential** — you can't improve what you don't measure
2. **Evals are hard** — getting them right requires care and iteration
3. **Evals can be gamed** — watch for Goodhart's Law
4. **Evals should match your domain** — generic rubrics fail specialized agents
5. **Evals need human oversight** — LLM judges are tools, not oracles
6. **Results need interpretation** — dig into the failures, not just the top line

---

## What's next

Now that we understand eval fundamentals, we'll build a proof-of-concept eval that:

1. Uses agent-aware rubrics (BI gets judged as BI, not as a generic chatbot)
2. Provides transparent reasoning (see exactly why something passed/failed)
3. Allows comparison (run same test cases through different rubric configs)
4. Generates actionable insights (not just scores, but patterns)

**Goal:** Demonstrate that a properly calibrated eval can distinguish between:
- Actual quality issues (agent needs to improve)
- Rubric misalignment (agent is fine, eval is wrong)
- Eval noise (neither is wrong, measurement is unreliable)

---

*"The first step in fixing a problem is knowing you have one. The second step is measuring it correctly."*


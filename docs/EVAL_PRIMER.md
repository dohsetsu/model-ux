# Evaluating AI agents: A practical primer

**Author:** Jason Bice  
**Date:** December 2025  
**Purpose:** Foundation knowledge for understanding, building, and interpreting AI evaluations

---

## What is an eval?

An **eval** (evaluation) is a systematic way to measure whether an AI system is doing its job well.

Think of it like a test — but instead of testing a student, you're testing an AI agent's outputs against some standard of "good."

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Input     │ ──► │  AI Agent   │ ──► │   Output    │
│  (question) │     │             │     │  (answer)   │
└─────────────┘     └─────────────┘     └─────────────┘
                                              │
                                              ▼
                                        ┌─────────────┐
                                        │    Eval     │
                                        │  (grading)  │
                                        └─────────────┘
                                              │
                                              ▼
                                        ┌─────────────┐
                                        │   Score     │
                                        │ (pass/fail) │
                                        └─────────────┘
```

---

## Why do evals matter?

Without evals, you're flying blind:

| Without Evals | With Evals |
|---------------|------------|
| "I think the agent is working well" | "The agent passes 87% of test cases" |
| "Users seem happy" | "Correctness is 92%, but completeness is only 68%" |
| "Let's ship it and see" | "This change improved relevance by 12%" |
| "Something broke but I don't know what" | "Regression detected: voice_tone dropped 15%" |

**Evals turn vibes into data.**

---

## Required components of an eval

Every eval needs these five things:

### 1. Test cases
The inputs you'll feed to the AI agent.

```
Example test case:
- User query: "What was my net profit last month?"
- Context: User is logged into QBO, has active P&L data
```

**Good test cases are:**
- Representative of real usage
- Diverse (cover edge cases, not just happy paths)
- Reproducible (same input + same test harness/setup — not necessarily same output, since LLMs are stochastic)

### 2. Ground truth
The "correct" answer to compare against.

```
Example ground truth:
"Your net profit for November 2024 was $12,450.00"
```

**Ground truth can be:**
- Human-written (gold standard, expensive)
- Extracted from source systems (e.g., actual QBO data)
- Generated by a reference model (faster, less reliable)

⚠️ **Ground truth quality determines eval quality.** Garbage in, garbage out.

### 3. Metrics
What dimensions of quality you're measuring.

| Metric | What it measures |
|--------|------------------|
| Correctness | Is the answer factually accurate? |
| Completeness | Does it fully address the question? |
| Relevance | Is the information useful and on-topic? |
| Voice/Tone | Does it sound appropriate for the context? |
| Latency | How fast did it respond? |
| Cost | How much did the API calls cost? |

**Choose metrics that matter for your use case.** A chatbot and a financial analyst tool need different metrics.

### 4. Evaluation method
How you determine pass/fail for each metric.

| Method | How it works | Pros | Cons |
|--------|--------------|------|------|
| **Exact match** | Output must equal ground truth exactly | Fast, deterministic | Too rigid for natural language |
| **Fuzzy match** | Semantic similarity above threshold | Handles paraphrasing | Threshold is arbitrary |
| **Rule-based** | Regex, keyword checks, format validation | Precise for specific patterns | Brittle, high maintenance |
| **LLM-as-a-judge** | Another AI grades the output | Flexible, nuanced | Expensive, can be inconsistent |
| **Human review** | People grade outputs | Gold standard for quality | Slow, expensive, doesn't scale |

### 5. Rubrics (for LLM-as-a-judge)
Instructions that tell the judge how to grade.

```
Example rubric for Correctness:
"Score TRUE if the response contains factually accurate information 
that matches the ground truth. Minor rounding differences are acceptable. 
Score FALSE if there are factual errors, wrong numbers, or contradictions."
```

**Rubrics are the soul of your eval.** Vague rubrics → inconsistent results.

---

## Deep dive: LLM-as-a-judge

Since this is the evaluation method most teams use at scale (including ours), it's worth understanding in detail.

### What it is (in plain language)

LLM-as-a-judge means **using one AI to grade another AI's work**.

Instead of a human reading every response and deciding "good" or "bad," you send the response to a separate LLM (the "judge") along with instructions (the "rubric") for how to grade it.

```
┌─────────────────┐
│  User Question  │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│   Your Agent    │ ──► Generates a response
└────────┬────────┘
         │
         ▼
┌─────────────────┐     ┌─────────────────┐
│    Response     │ ──► │   Judge LLM     │ ──► Pass/Fail + Reasoning
└─────────────────┘     │  (+ Rubric)     │
                        └─────────────────┘
```

The judge sees:
- The original question
- The agent's response
- The ground truth (what the "right" answer should be)
- The rubric (instructions for how to evaluate)

And returns:
- A verdict (TRUE/FALSE, or a score)
- Reasoning (why it made that decision)

### Why teams use it

| Challenge | LLM-as-a-judge helps because... |
|-----------|--------------------------------|
| **Scale** | Humans can review ~50 responses/hour. LLMs can review thousands. |
| **Consistency** | Humans get tired, distracted, and vary day-to-day. LLMs don't (as much). |
| **Nuance** | Unlike regex or exact-match, LLMs can understand paraphrasing, context, and intent. |
| **Cost** | Human review at scale is expensive. API calls are cheap by comparison. |
| **Speed** | Get results in minutes, not days. |

### The tradeoff you're making

When you use LLM-as-a-judge, you're trading **gold-standard accuracy** for **scale and speed**.

```
Human review:     High accuracy, low scale, slow, expensive
LLM-as-a-judge:   Good-enough accuracy, high scale, fast, cheap
```

This is usually a good tradeoff — but only if you:
1. Write good rubrics
2. Spot-check with humans regularly
3. Understand the judge's failure modes

### How the judge can be wrong

This is the critical part. **The judge is not an oracle.** It's another LLM with its own biases and limitations.

#### Position bias
If you show the judge two responses (A and B) and ask "which is better?", it may prefer whichever one you showed first — regardless of actual quality.

**Mitigation:** Randomize the order, or run both orderings and check for consistency.

#### Length bias
Judges often prefer longer, more detailed responses — even when brevity is better.

```
Response A: "Your net profit was $12,450."
Response B: "Based on your Profit & Loss statement for the period ending 
            November 30, 2024, your net profit was $12,450.00. This represents 
            the difference between your total revenue and total expenses..."

Judge might prefer B, even if A is exactly what the user needed.
```

**Mitigation:** Include examples in your rubric showing that concise answers can score high.

#### Style matching
Judges may reward outputs that "sound like" their own training data — formal, polished, verbose.

**Mitigation:** Use few-shot examples in your rubric that show the *actual* style you want.

#### Sycophancy
Some judges are reluctant to give harsh scores. They'll find reasons to pass marginal responses.

**Mitigation:** Include clear failure examples in your rubric. Make it "safe" to say FALSE.

#### Inconsistency across runs
The same response can get different scores on different runs (even with temperature=0, due to infrastructure variations).

**Mitigation:** Run critical evals multiple times and look at distributions, not single scores.

### What makes a good rubric (for LLM-as-a-judge)

The rubric is everything. A vague rubric produces vague results.

**Bad rubric:**
```
"Score TRUE if the response is good and helpful."
```
(What does "good" mean? The judge will make up its own definition.)

**Better rubric:**
```
"Score TRUE if the response:
1. Directly answers the user's question
2. Contains no factual errors when compared to ground truth
3. Uses clear, professional language

Score FALSE if:
- The response contains incorrect numbers or dates
- The response answers a different question than asked
- The response includes information not supported by the data"
```

**Best rubric (with examples):**
```
"Score TRUE if the response directly answers the question with accurate data.

Example of a TRUE response:
User: "What was my net profit last month?"
Response: "Your net profit for November was $12,450."
Why TRUE: Directly answers the question with the correct figure.

Example of a FALSE response:
User: "What was my net profit last month?"
Response: "Your business is doing well! You should consider investing in marketing."
Why FALSE: Does not answer the question asked.

Now evaluate the following..."
```

**The pattern:** Definition → Detection criteria → Few-shot examples

### When to trust the judge (and when not to)

| Trust the judge when... | Don't trust the judge when... |
|-------------------------|-------------------------------|
| Rubric is specific and has examples | Rubric is vague or generic |
| You've validated against human judgment | You've never spot-checked |
| Results are consistent across runs | Same response gets different scores |
| Judge reasoning makes sense | Judge reasoning is circular or wrong |
| Metric is objective (correctness, format) | Metric is highly subjective (creativity, empathy) |

### The human-in-the-loop pattern

Best practice is **not** "humans OR LLM" — it's "LLM first, humans verify":

```
1. LLM judges all responses (fast, cheap)
2. Humans review a sample of:
   - Random passes (are these actually good?)
   - Random fails (are these actually bad?)
   - Edge cases (where confidence is low)
3. Track human/LLM agreement rate over time
4. If agreement drops, investigate rubric or judge issues
```

This gives you scale AND quality assurance.

---

## Types of evals (and how to choose)

There isn't one "right" eval. The right eval depends on:
1. What question you're trying to answer
2. What you can verify
3. The risk of being wrong

### By what you're measuring (most useful for product teams)

| Eval family | What it answers | Best when… | Common gotchas |
|-------------|-----------------|------------|----------------|
| **Reference/ground-truth scoring** (exact match, structured fields, unit tests) | "Is it correct?" | You can define a verifiable output (numbers, JSON schema, classification label) | Can miss *helpfulness*; over-optimizes to format |
| **Rubric-based human grading** | "Is it good for users?" | Quality is subjective (clarity, tone, reasoning, UX) | Needs rater training + calibration or results drift |
| **Pairwise preference** (A vs B) | "Which version is better?" | You're comparing prompts/models and want high rater reliability | Doesn't tell you *why* without follow-up tagging |
| **LLM-as-a-judge** | "Can we scale grading?" | You need volume and can accept periodic human audits | Judges have biases (position, length) and need controls |
| **Instruction-following / alignment** | "Did it follow the rules?" | You have documented system instructions or policies | Must keep rubrics synced with actual instructions |
| **Multi-turn scenario evals** | "Does it hold up across a conversation?" | Your product is an assistant/agent, not one-shot Q&A | Expensive; requires careful scenario design |
| **Tool/environment-based evals** | "Can the system complete the task?" | Success can be verified by tool outputs (e.g., retrieved doc, computed totals) | Test harness must be stable; tool outputs can drift |
| **Routing/orchestration evals** (supervisor) | "Did we pick the right agent + tools?" | You have multiple agents and a router/supervisor | Needs trace logging + labels; "correct agent" can be fuzzy |
| **Robustness/consistency evals** | "Is it stable under paraphrase and repetition?" | You care about reliability and not being "flaky" | Must sample multiple runs; single run is misleading |
| **Adversarial/red-team safety evals** | "Can users push it into unsafe behavior?" | High-stakes or compliance-sensitive domains | Needs continuous refresh; attackers adapt |

### Quick decision guide: "Which eval should I use?"

**If you can verify the result mechanically** (numbers, categories, structured output):
→ Start with **reference-based** + **tool-verification** evals

**If you're judging experience quality** (helpfulness, tone, clarity):
→ Use **rubric-based human grading** early
→ Then scale with **LLM-as-a-judge + human spot checks**

**If you're comparing two versions** (prompt, model, agent policy):
→ Prefer **pairwise preference** over "absolute 1–5" ratings for reliability
→ For large comparisons, use tournament/pairwise structures (more stable rankings)

**If the system is agentic** (router + tools + multi-turn):
→ Always include **multi-turn scenarios** + **trace-based checks** (routing correctness, tool calls, grounding)

**If harm/compliance risk is high:**
→ Add **adversarial/red-team** evals and treat failures as release blockers

### By automation level

| Type | Description | When to use |
|------|-------------|-------------|
| **Manual** | Humans review every output | Early development, high-stakes decisions |
| **Semi-automated** | LLM grades, humans spot-check | Regular testing, moderate volume |
| **Fully automated** | No human in the loop | CI/CD pipelines, high volume |

### By timing (offline vs online)

| Type | Description | When to use |
|------|-------------|-------------|
| **Offline** | Run on historical data, batch mode | Before deployment, regression testing |
| **Online** | Run on live traffic, real-time | Production monitoring, A/B tests |

#### What's the difference?

**Offline evals** run on a fixed dataset — usually historical queries or curated test cases — *before* changes hit production.

```
You have: 500 test cases from last month's traffic
You run: New prompt version against all 500
You get: Scores (correctness: 87%, voice: 72%, etc.)
You decide: Ship it or keep iterating
```

**Online evals** run on live traffic — real users asking real questions — *after* changes hit production.

```
You deploy: New prompt version to 10% of users
You measure: Success rate, user satisfaction, error rates
You compare: Does 10% group perform better than control?
You decide: Roll out to 100% or roll back
```

#### Why this matters

| Offline evals are good at... | Online evals are good at... |
|------------------------------|----------------------------|
| Fast iteration (minutes, not days) | Real-world validation |
| Catching regressions before users see them | Catching issues your test set missed |
| Controlled comparisons (same inputs) | Measuring actual user experience |
| Testing edge cases you thought of | Discovering edge cases you didn't |

#### The gap between them

Your offline test set is a **sample** of reality. It has blind spots:
- Queries users ask that you didn't include
- Seasonal patterns (tax season vs. mid-year)
- New features or data that didn't exist when you built the set
- Real user behavior (follow-ups, clarifications, mistakes)

**Example of the gap:**
- Offline eval: 92% correctness on 500 test cases ✅
- Online reality: Users are asking about a new report type that wasn't in your test set → errors spike

#### The right mental model

| Stage | Eval type | What it tells you |
|-------|-----------|-------------------|
| Development | Offline | "This might work" |
| Pre-launch | Offline (regression) | "We didn't break anything we know about" |
| Launch | Online (A/B test) | "This actually works for real users" |
| Ongoing | Online (monitoring) | "It's still working" |

**Use both.** Offline evals are your *lab tests* — fast iteration, regression prevention. Online evals are your *real-world validation* — they catch things offline sets don't. Online doesn't replace offline; it verifies it.

---

## Which eval for which system? (A cookbook)

Different system architectures have different failure modes. Here's a guide to matching eval approach to system type.

### RAG (Retrieval-Augmented Generation)

**What it is:** The agent retrieves documents/data, then generates a response grounded in what it retrieved. (Example: Search agent pulling help articles)

**Key failure modes:**
- Retrieved wrong documents
- Retrieved right documents but ignored them
- Hallucinated beyond what was retrieved
- Correctly retrieved but misinterpreted

**Evals to prioritize:**

| Eval | What it checks | Example |
|------|----------------|---------|
| **Retrieval quality** | Did it pull the right docs? | "For 'how to void a check', did it retrieve the check-voiding article?" |
| **Grounding/faithfulness** | Is the response supported by retrieved content? | "Does every claim map to something in the retrieved docs?" |
| **Answer correctness** | Is the final answer right? | "Is the procedure accurate?" |
| **Attribution** | Does it cite sources correctly? | "If it says 'according to X', is that accurate?" |

**Pro tip:** You need to log what was retrieved to evaluate retrieval quality. If you only see the final answer, you can't tell if retrieval failed.

---

### Multi-agent orchestrator (Supervisor → Sub-agents)

**What it is:** A supervisor/router decides which specialized agent handles the query, then that agent responds. (Example: Omni routing to BI, Search, Bookkeeper, etc.)

**Key failure modes:**
- Routed to wrong agent
- Routed correctly but agent failed
- Agent succeeded but response poorly synthesized
- Routing ambiguity (multiple agents could handle it)

**Evals to prioritize:**

| Eval | What it checks | Example |
|------|----------------|---------|
| **Routing correctness** | Did supervisor pick the right agent? | "BI question → should route to BI agent" |
| **Agent-specific quality** | Did the chosen agent do its job well? | "Given it went to BI, was the BI response correct?" |
| **Handoff quality** | Was context preserved across agents? | "Did the receiving agent have what it needed?" |
| **End-to-end correctness** | Was the final answer right? | "Regardless of path, is the answer correct?" |

**Pro tip:** Log the routing decision and evaluate it separately. A wrong route that produces an okay answer is still a bug.

---

### Tool-using agent

**What it is:** The agent can call external tools (APIs, databases, calculators) to get information or take actions. (Example: BI agent calling QBO APIs)

**Key failure modes:**
- Called wrong tool
- Called right tool with wrong arguments
- Tool returned error, agent didn't handle gracefully
- Tool returned data, agent misinterpreted it

**Evals to prioritize:**

| Eval | What it checks | Example |
|------|----------------|---------|
| **Tool selection** | Did it call the right tool? | "For 'net profit', did it call the P&L endpoint?" |
| **Argument correctness** | Were the parameters right? | "Did it pass the correct date range?" |
| **Error handling** | Does it fail gracefully? | "If the API errors, does it say so vs. hallucinate?" |
| **Result interpretation** | Did it correctly use the tool output? | "Did it read the JSON correctly?" |

**Pro tip:** Log tool calls (name, arguments, response) and evaluate the trace, not just the final answer.

---

### Simple chatbot / Q&A

**What it is:** Direct question → answer, no retrieval or tools. (Example: FAQ bot, simple assistant)

**Key failure modes:**
- Wrong answer
- Off-topic response
- Inappropriate tone
- Hallucination

**Evals to prioritize:**

| Eval | What it checks | Example |
|------|----------------|---------|
| **Correctness** | Is the answer factually right? | Standard accuracy check |
| **Relevance** | Does it answer the question asked? | "Did it address the actual query?" |
| **Voice/tone** | Is it appropriate for context? | Brand voice, formality level |
| **Harmlessness** | Does it avoid unsafe content? | Policy compliance |

**Pro tip:** This is where rubric-based LLM-as-a-judge works best — outputs are self-contained and quality is relatively easy to judge.

---

### Classification / Intent routing

**What it is:** Categorize input into predefined buckets. (Example: "Is this a billing question or a technical question?")

**Key failure modes:**
- Misclassification
- Low confidence handled poorly
- Edge cases between categories

**Evals to prioritize:**

| Eval | What it checks | Example |
|------|----------------|---------|
| **Accuracy** | Did it classify correctly? | Precision, recall, F1 by category |
| **Confusion analysis** | Where does it get confused? | "Often mislabels X as Y" |
| **Confidence calibration** | Is confidence score meaningful? | "When it says 90% confident, is it right 90% of the time?" |
| **Edge case handling** | What about ambiguous inputs? | "Query that could be either category" |

**Pro tip:** This is one of the few cases where exact-match / ground-truth eval works well. You have labels, you can measure accuracy directly.

---

### Quick reference: System → Eval priority

| System type | Primary eval focus | Don't forget |
|-------------|-------------------|--------------|
| **RAG** | Retrieval quality, grounding | Log what was retrieved |
| **Multi-agent** | Routing correctness, agent-specific quality | Log routing decisions |
| **Tool-using** | Tool selection, argument correctness | Log tool traces |
| **Simple chatbot** | Correctness, voice/tone | Rubric-based works well here |
| **Classification** | Accuracy, confusion matrix | Ground-truth eval is tractable |

---

### By scope

| Type | Description | Example |
|------|-------------|---------|
| **Unit eval** | Test one component in isolation | "Does the date parser work?" |
| **Integration eval** | Test components working together | "Does BI + QBO data return correct P&L?" |
| **End-to-end eval** | Test full user journey | "Can a user get their tax summary?" |

---

## The eval crisis (an industry-wide problem)

The AI industry is grappling with what researchers call the **LLM Evaluation Crisis** — a fundamental challenge in reliably assessing what large language models can actually do. This isn't a problem with any specific eval or team; it's a systemic issue that the entire field is working to solve.

Understanding this crisis is essential for Model UX practitioners because it shapes how we should think about building, interpreting, and improving evaluations.

### The core problem

Traditional software testing assumes deterministic behavior: same input → same output. LLMs break this assumption. They're often stochastic and highly sensitive to context — even small changes in prompting, retrieved context, or infrastructure can change outputs. This makes conventional testing approaches unreliable.

### Key dimensions of the crisis

#### 1. Non-determinism and non-stationarity
LLMs produce different outputs for the same prompt across runs. This isn't a bug — it's how they work. But it means:
- The same test case can pass today and fail tomorrow
- "Flaky" results aren't always a sign of bad tests
- Statistical approaches are necessary, but add complexity

For agent systems (with routing, tools, and retrieval), there's an additional layer: **non-stationarity**. The environment itself changes — retrieved documents get updated, tool outputs vary, routing logic evolves, system prompts change. So "same input" doesn't mean "same conditions." This adds extra sources of drift that pure non-determinism doesn't capture.

#### 2. Benchmark overfitting and contamination
Models can learn to perform well on public benchmarks without truly understanding the underlying task. They may:
- Memorize patterns from training data that overlap with test sets
- Generate answers that "look right" without genuine reasoning
- Score high on benchmarks but fail on slight variations

Researchers call this **benchmark contamination** or **data leakage** — when training data overlaps with or "leaks" into test sets. This is why some benchmarks now use closed/held-out test sets to reduce contamination.

**Why it matters:** A model that "passes" an eval may still fail in real-world scenarios that weren't in the test set.

#### 3. Lack of self-awareness
LLMs often can't identify their own limitations. They may:
- Provide confident answers to questions they don't actually know
- Fail to signal uncertainty when they should
- "Hallucinate" plausible-sounding but incorrect information

**Why it matters:** An eval that only checks "is this answer correct?" misses "does the model know when it doesn't know?"

#### 4. Multi-turn inconsistency
Models may fail to maintain consistent reasoning across a conversation:
- Contradicting earlier statements
- Abandoning correct answers when challenged
- Losing context as conversations grow longer

**Why it matters:** Single-turn evals don't catch problems that only emerge in realistic multi-step interactions.

#### 5. Fragile reasoning
Models may rely on superficial patterns rather than deep understanding:
- Getting the right answer for the wrong reasons
- Failing when problems are rephrased
- Showing "brittleness" under minor perturbations

**Why it matters:** High eval scores can mask fundamental reasoning gaps.

#### 6. Evaluation difficulty itself
Scoring natural language is inherently hard:
- "Good" is subjective and context-dependent
- Benchmarks lag behind rapidly evolving capabilities
- Using LLMs to judge LLMs introduces its own biases

**LLM-as-a-judge has well-documented failure modes:**
- **Position bias:** Judges may prefer whichever answer appears first (or second) regardless of quality
- **Length bias:** Judges often favor longer, more "polished" answers even when brevity is better
- **Style matching:** Judges may reward outputs that sound like their own training data

These aren't theoretical — they're observed in research and practice. This is why "the judge said so" isn't sufficient justification.

#### 7. Construct validity (are we measuring what matters?)
Even if your benchmark is stable and well-designed, it might not measure what you actually care about in production. A high score on "correctness" doesn't guarantee "helps users complete accounting tasks safely."

This is the gap between **benchmark performance** and **real-world usefulness** — and it's why human validation remains essential.

### What this means for practitioners

The eval crisis doesn't mean evals are useless — it means we need to be thoughtful about:

1. **Designing evals** that account for non-determinism, non-stationarity, and real-world scenarios
2. **Interpreting results** with appropriate skepticism (high scores ≠ solved problem)
3. **Building rubrics** that match the specific context and use case
4. **Validating construct validity** — does your eval actually predict what users care about?
5. **Combining methods** rather than relying on any single approach

This is why Model UX exists as a discipline: we need people who understand both the language/UX side and the evaluation/measurement side to bridge this gap.

---

## Eval best practices (what works in 2025–2026)

These practices directly address the eval crisis failure modes (variance, leakage, judge bias, drift).

### 1. Version everything

An eval is only interpretable if you can reproduce the setup:
- Model + decoding settings (temperature, etc.)
- System prompt / policies
- Tools and tool schemas
- Retrieval configuration (and ideally, a snapshot of retrieved corpora)
- Judge prompt + judge model (if using LLM-as-a-judge)
- Dataset version + rubric version

**Why it matters:** "Scores changed" is meaningless without knowing what else changed.

### 2. Separate datasets by purpose

This prevents accidental "teaching to the test" and gives you a trustworthy progress signal:

| Dataset | Purpose | Rules |
|---------|---------|-------|
| **Dev set** | Iterate freely | Expect to overfit; that's fine |
| **Regression set** | Catch breakages | Frozen; run in CI |
| **Holdout set** | Measure real progress | Locked; only used for "are we actually improving?" |

Keeping some evaluation data closed/held-out reduces benchmark contamination.

### 3. Measure more than accuracy (multi-metric thinking)

For real products, accuracy trades off with other factors:
- Robustness (does it fail gracefully?)
- Safety (can it be pushed into harmful outputs?)
- Efficiency (latency, cost)
- User experience (clarity, tone, helpfulness)

A single "overall score" hides these tradeoffs. Break it down.

### 4. Treat scores as distributions, not single numbers

LLM outputs vary. Best practice:
- Run multiple samples for critical scenarios
- Report pass-rate **and** variance (or confidence intervals)
- Explicitly test consistency/robustness

A single eval run can be misleading. Statistical thinking is required.

### 5. If you use LLM-as-a-judge, control for known failure modes

**Minimum controls:**
- Randomize A/B order to reduce **position bias**
- Prefer **pairwise** comparisons for subjective quality
- Periodically audit with humans and track agreement
- Document the judge model version (judge behavior changes with model updates)

**Don't assume the judge is objective.** It has its own biases.

### 6. For agents: log traces and **evaluate the trace**, not just the final answer

Agent systems fail in ways that final-answer grading can't see:
- Wrong routing (supervisor picked the wrong specialist)
- Wrong tool or wrong tool arguments
- Missing retrieval when it should have grounded the answer
- Correct answer but broken reasoning

**Best practice:** Store and score the execution trace (routing decision, tool calls, retrieved evidence) — not just the output.

### 7. Use online measurement to validate offline wins

Offline eval improvements can be illusory. Online metrics catch:
- Behavior changes in real distributions (not just your test set)
- Latency/cost regressions
- Risk exposure changes
- User satisfaction (the ultimate ground truth)

**Offline tells you "it might work." Online tells you "it does work."**

### 8. Standardize safety testing

Don't rely on ad-hoc jailbreak prompts. Use structured, repeatable red-teaming suites and keep them updated. Treat safety failures as release blockers, not just metrics to track.

---

## How to read eval results

### Don't just look at the top-line number

❌ "We got 87% — ship it!"

✅ "We got 87% overall. Let's break that down..."

### Break down by dimension

| Metric | Score | Interpretation |
|--------|-------|----------------|
| Correctness | 92% | Strong — factual accuracy is good |
| Completeness | 68% | **Weak — many partial answers** |
| Relevance | 85% | Acceptable |
| Voice/Tone | 71% | **Needs attention** |

### Break down by segment

| Segment | Correctness | Notes |
|---------|-------------|-------|
| Simple queries | 96% | Easy cases handled well |
| Multi-part questions | 78% | **Struggles with complexity** |
| Edge cases | 61% | **Known weakness** |

### Look at the failures

The most valuable data is in the FALSE rows:
- What patterns do you see?
- Are failures random or systematic?
- Is the eval wrong, or is the agent wrong?

### Check for noise

Ask yourself:
- Do similar responses get similar scores?
- If you re-ran the eval, would results change significantly?
- Are there obvious contradictions in the reasoning?

---

## How to adjust evals

### When to adjust rubrics

✅ **Good reasons:**
- Rubric doesn't match user expectations
- Rubric creates perverse incentives
- Agent has legitimate domain-specific needs

❌ **Bad reasons:**
- "We need to hit 85%"
- "The failures look bad on the dashboard"
- "Other team is complaining"

### How to adjust rubrics

1. **Document the problem** — specific examples of unfair failures
2. **Propose specific changes** — not "make it easier" but "accounting terms should be acceptable when..."
3. **Test the change** — run both rubrics, compare results
4. **Validate with humans** — does the new rubric match expert judgment?
5. **Monitor for regression** — did you accidentally break something else?

### Agent-aware adjustments

Instead of one rubric for all agents:

```
IF agent == "Business_Intelligence":
    Apply BI-specific rubric
    - Accounting terminology: EXPECTED (not jargon)
    - Structured format: ACCEPTABLE
    - Comprehensive detail: GOOD (not verbose)
ELSE:
    Apply standard rubric
```

This lets each agent be judged by appropriate standards while keeping the eval framework unified.

---

## How to interpret results

### Correlation ≠ Causation

"We changed the prompt and correctness went up 5%"

But did the prompt change cause it? Or:
- Did test data change?
- Did the judge model change?
- Is it just noise?

**Always compare apples to apples:** same test cases, same judge, same rubrics.

### Beware of ceiling effects

If you're at 95% correctness, a 1% improvement is actually huge — you fixed 20% of your remaining errors.

If you're at 60%, a 5% improvement might just be noise.

### Trust but verify

LLM-as-a-judge is a tool, not an oracle.

**Spot-check the reasoning:**
- Does the judge's explanation make sense?
- Would a human agree with this verdict?
- Are there obvious errors in the evaluation?

### Results are a starting point, not an endpoint

Eval results tell you *where* to look, not *what* to do.

```
Eval says: "Completeness is failing"
     ↓
Investigation: "50% of failures are date interpretation issues"
     ↓
Root cause: "Agent and ground truth define 'last quarter' differently"
     ↓
Fix: "Standardize date interpretation logic"
```

---

## Quick reference: Eval red flags

| Red Flag | What it might mean |
|----------|-------------------|
| All metrics improved after "tuning" | Judge was made more lenient, not agents improved |
| Different agents have identical improvements | Global change, not targeted fixes |
| Same content gets different verdicts | Eval has noise/inconsistency |
| Correctness unchanged but subjective metrics jumped | Rubrics loosened, facts untouched |
| Team can't explain what changed | Changes weren't documented or understood |
| Failures dismissed as "eval problems" | Culture of explaining away rather than fixing |
| No human validation of judge | Blind trust in automated grading |

---

## Summary

1. **Evals are essential** — you can't improve what you don't measure
2. **Evals are hard** — getting them right requires care and iteration
3. **Evals can be gamed** — watch for Goodhart's Law
4. **Choose the right eval type** — different questions need different approaches
5. **Evals should match your domain** — generic rubrics fail specialized agents
6. **Evals need human oversight** — LLM judges are tools, not oracles
7. **Version everything** — reproducibility is non-negotiable
8. **For agents, evaluate traces** — final-answer grading misses routing and tool failures
9. **Results need interpretation** — dig into the failures, not just the top line

---

## What's next

Now that we understand eval fundamentals, we'll build a proof-of-concept eval that:

1. Uses agent-aware rubrics (BI gets judged as BI, not as a generic chatbot)
2. Provides transparent reasoning (see exactly why something passed/failed)
3. Allows comparison (run same test cases through different rubric configs)
4. Generates actionable insights (not just scores, but patterns)

**Goal:** Demonstrate that a properly calibrated eval can distinguish between:
- Actual quality issues (agent needs to improve)
- Rubric misalignment (agent is fine, eval is wrong)
- Eval noise (neither is wrong, measurement is unreliable)

---

*"The first step in fixing a problem is knowing you have one. The second step is measuring it correctly."*

